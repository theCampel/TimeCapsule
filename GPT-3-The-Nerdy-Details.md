### ChatGPT - The Nerdy Details:
Experts of specific fields realised that ChatGPT (GPT-3.5) was really just blowing hot air when it spoke about a topic. Too often it made up information (_hallucination_ - a problem I expect to be a thing of the past by 2030), broke past its guardrails (gave harmful information if prompted correctly) or even just forgot what what was discussed almost immediately after it was said (short _context window_). 

A few months later, GPT-4 rolled around… for $20 a month. GPT-4 almost completely eradicated the first 2 main errors GPT-3.5 had (GPT-4 also had a much longer context window, about 25k words). The problem? Because it’s paid, very few consumers see it worth the investment when you can get “_good enough_” answers from GPT-3.5 for free (businesses use the GPT-3.5 fine-tuned models with great success).

 

ChatGPT works using a type of algorithm called a _Transformer_. By slightly optimising the original algorithm and feeding it _ungodly amounts of data and computational power_ (as well as getting real humans to rank the outputs), OpenAi was able to produce ChatGPT (after some promising successes from GPT-1 and GPT-2). 

The name of the game is just feeding transformers _more compute_ (computational power). For the past 5 years, the amount of compute power used to train these models has increased _by a factor of 2 every 6 months_. In other words, we **_double_** the amount of **_computational power_** **_every 6 months_**. What’s most astonishing, it doesn’t look like this is going to stop. So by 2030, we will have _2^14_ times the compute we have today to train our most advanced models. 
